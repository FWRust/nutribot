{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FWRust/nutribot/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tykCqhfS6fLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "Gpv5xDKcN3X0",
        "outputId": "9fc089dd-f287-408c-a193-0af319b8a715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
            "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5473177575\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "WARNING:aiogram.dispatcher.dispatcher:Polling is stopped.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-507280cdccce>\u001b[0m in \u001b[0;36m<cell line: 531>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    115\u001b[0m             else None)\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import asyncio\n",
        "from aiogram import Bot, Dispatcher, executor, types\n",
        "from aiogram.dispatcher import FSMContext\n",
        "from aiogram.dispatcher.filters.state import State, StatesGroup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nest_asyncio\n",
        "from aiogram.contrib.fsm_storage.memory import MemoryStorage\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import (\n",
        "     AutoModelForCausalLM,\n",
        "     AutoTokenizer,\n",
        "     pipeline,\n",
        "     GPTQConfig\n",
        " )\n",
        "from google.colab import drive\n",
        "from googletrans import Translator\n",
        "nest_asyncio.apply()\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "drive = GoogleDrive(gauth)\n",
        "storage = MemoryStorage()\n",
        "class UserStates(StatesGroup):\n",
        "  start_bot = State()\n",
        "  to_questionary = State()\n",
        "  to_analysis = State()\n",
        "  after_analysis = State()\n",
        "  save_file = State()\n",
        "  to_process_analysis = State()\n",
        "  to_AI = State()\n",
        "token = '6330602631:AAGD-y1wboKQSXOkyJUWlV7UVXmNRVgPN90'\n",
        "bot = Bot(token)\n",
        "dp = Dispatcher(bot, storage=storage)\n",
        "MAX_QUESTIONS = 33\n",
        "# начало нейронки\n",
        "\n",
        "translator = Translator()\n",
        "torch.cuda.is_available()\n",
        "\n",
        "### Инициализация модели ######################################################################################\n",
        "# Эта ячейка загружает модель в видеокарту, ни в коем случае ее нельзя запускать несколько раз за одну сессию,\n",
        "# иначе в видюху загрузится несколько моделей и в ней начнет закачниватся видеопамять,\n",
        "# если все таки нужно повторно запустить код, то сначала перезапустите среду чтобы освободить\n",
        "# память от прошлой модели.\n",
        "# По умолчанию модель занимает не более 10 ГБ VRAM, если занято больше значит вы что то делаете не так\n",
        "#####################################################################################################################\n",
        "\n",
        "base_model_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "adapter_model = '/content/drive/MyDrive/checkpoint-240'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
        "                                              device_map={\"\": 0},\n",
        "                                              quantization_config=GPTQConfig(bits=4,disable_exllama=True))\n",
        "model = PeftModel.from_pretrained(model, adapter_model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "### Параметры модели ###\n",
        "\n",
        "pipe = pipeline('text-generation',model=model,tokenizer=tokenizer,\n",
        "                 max_new_tokens=100, # Максимальная длина ответа, сильное снижение может привести к обрезанию ответов\n",
        "                 do_sample=True,   # не трогать\n",
        "                 temperature=0.1,  # рандомность ответа, чем больше это число тем более неожиданные и креативные ответы будет давать модель, если число маленькое то ответы будут однообразными но уверенными.\n",
        "                 top_p=0.95, #  тоже рандомность, чем меньше тем больше модель будет отходить от контекста.\n",
        "                 top_k=40, # аналагично параметру выше, не рекомендуется менять.\n",
        "                 repetition_penalty=1.15  # штраф за повторение, чем выше тем меньше модель будет повторять одни и те же слова/предложения, слишком высокое значение может привести к бреду/галлюцинациям\n",
        "                )\n",
        "\n",
        "# # ### Системный промпт, в нем вы обьясняете боту кто он по жизни и в чем его цель, что ему стоит делать а что нет, можно свободно изменять.\n",
        "system_prompt = \"You are a helpful, respectful and honest assistant, you answer questions regarding food supplements and vitamins. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Answer the topic of the question, do not deviate from the topic of the question, do not philosophize, be precise and concise, use less than 150 words. Don't use any diagnoses or statistics in your answer. Use as few medical terms as possible. Do not ask questions back. Do not advise any vitamins.\"\n",
        "\n",
        "\n",
        "\n",
        "#конец нейронки\n",
        "\n",
        "# Ищем id файла на диске\n",
        "def find_file():\n",
        "  file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "  for file in file_list:\n",
        "    if file['title'] == 'user_data.csv':\n",
        "        id = file['id']\n",
        "  return id\n",
        "\n",
        "\n",
        "# Скачиваем файл с этим id\n",
        "def download_file(id):\n",
        "  download_db = drive.CreateFile({'id': f'{id}'})\n",
        "  download_db.GetContentFile('user_data.csv')\n",
        "  db = pd.read_csv('user_data.csv', encoding='utf-8', sep=',')\n",
        "  return db, download_db\n",
        "\n",
        "\n",
        "# загружаем файл на диск\n",
        "def upload_file():\n",
        "  file_to_upload = drive.CreateFile({'title': 'user_data.csv'})\n",
        "  file_to_upload.SetContentFile('user_data.csv')\n",
        "  file_to_upload.Upload()\n",
        "\n",
        "\n",
        "# Берем результаты опросника и на основе них выводим возможные дефициты:\n",
        "\n",
        "\n",
        "\n",
        "# Получаем информацию о результатах опроса конкретного пользователя\n",
        "async def get_user_data(user_id, db, specific_id):\n",
        "  hypothyroidism = db['Г'].to_list()[specific_id]\n",
        "  insulinresistance = db['ИР'].to_list()[specific_id]\n",
        "  irondeficit = db['ЖД'].to_list()[specific_id]\n",
        "  return hypothyroidism,insulinresistance,irondeficit\n",
        "\n",
        "\n",
        "# Получаем id пользователя в базе пользователей\n",
        "async def get_user_id_in_base(user_id):\n",
        "  with open('user_data.csv') as f:\n",
        "    db = pd.read_csv('user_data.csv')\n",
        "    user_ids = list(db['user_id'])\n",
        "    if user_id in user_ids:\n",
        "      specific_id = user_ids.index(user_id)\n",
        "    else:\n",
        "      specific_id = None\n",
        "  return specific_id, db\n",
        "\n",
        "async def print_instructions(message: types.Message, markup=None):\n",
        "  await bot.send_message(message.chat.id, \"Подготовка к анализам:\\n\"\n",
        "                      \"Общие правила подготовки и сдачи анализов крови на витамины и минералы:\\n\"\n",
        "                      \"●прийти в лабораторию заранее, чтобы привести в порядок эмоциональное и физическое состояние\\n\"\n",
        "                      \"●с момента последнего приема пищи должно пройти не менее восьми часов\\n\"\n",
        "                      \"●с утра разрешается пить воду, только чистую без добавок\\n\"\n",
        "                      \"●за неделю до сдачи (примерно) отказаться от употребления содержащих спирт напитков (в Т.Ч.\\n\"\n",
        "                      \"аптечной продукции)\\n\"\n",
        "                      \"●в день сдачи крови желательно воздерживаться от курения\\n\"\n",
        "                      \"●нельзя совмещать дни посещения лаборатории и физиотерапевтических (аппаратных) процедур исключить интенсивные физические нагрузки\\n\"\n",
        "                      \"Если Вы находитесь в состоянии сильных эмоциональных переживаний, возможно получение искаженных результатов.\")\n",
        "\n",
        "\n",
        "# Подгружаем все необходимые файлы\n",
        "user_agreement = drive.CreateFile({'id': '1zPrp6ou173pwdThjG4Uq_Q9087l9pABp'})  # Его id на гугл диске\n",
        "user_agreement.GetContentFile('Пользовательское соглашение.pdf')  # название\n",
        "\n",
        "process_data = drive.CreateFile({'id': '1SYV6HZ344MOZp7gnu6OWAySjIzXexi_9'})\n",
        "process_data.GetContentFile('Обработка_персональных_данных.pdf')\n",
        "\n",
        "questions = drive.CreateFile({'id': '1bKuEAn1piaz6HhLK2QfeVX75zGg6CqC5'})\n",
        "questions.GetContentFile('questions.txt')\n",
        "\n",
        "zinc = drive.CreateFile({'id': '1xGnABMJRE27t3syyG1DaIx7k3XCgXXIp'})\n",
        "zinc.GetContentFile('Цинк.pdf')\n",
        "\n",
        "chrome = drive.CreateFile({'id': '1wyWfmYDdoTZwD0ZwCAWKY74Q8CNVMBgY'})\n",
        "chrome.GetContentFile('Хром.pdf')\n",
        "\n",
        "acid = drive.CreateFile({'id': '1g80MGqMXA2PmAe4vxjchqGe09dMy3pjJ'})\n",
        "acid.GetContentFile('Фолиевая кислота.pdf')\n",
        "\n",
        "test_results = drive.CreateFile({'id': '1x1UoV6Zmn6Ch0P4wXveZ3i-kGv_h_h0k'})\n",
        "test_results.GetContentFile(\"Показания.xlsx\")\n",
        "\n",
        "selen = drive.CreateFile({'id': '1HziBuXzpdVaFr8qWLbeyLjgEDw_X72bj'})\n",
        "selen.GetContentFile('Селен.pdf')\n",
        "\n",
        "copper = drive.CreateFile({'id': '1O1E_Y2lkjXixCo3lfemfbSGchzEAVsEv'})\n",
        "copper.GetContentFile('Медь.pdf')\n",
        "\n",
        "magnium = drive.CreateFile({'id': '18t-tjAuHyqX5PcrMXMlSfvDkIWlRgYjp'})\n",
        "magnium.GetContentFile('Магний.pdf')\n",
        "\n",
        "b12 = drive.CreateFile({'id': '1vb4_lCF5Nja10i3nJzlpUfPuC5D5I5Du'})\n",
        "b12.GetContentFile('Кобаламин (B12).pdf')\n",
        "\n",
        "iodine = drive.CreateFile({'id': '1xK2AvcWMfcEEKThA9j5i7pVdQ2y3RUwl'})\n",
        "iodine.GetContentFile('Йод.pdf')\n",
        "\n",
        "fats = drive.CreateFile({'id': '1UPweqKiIN6qeItcxM4swvRcpaZM8ywXI'})\n",
        "fats.GetContentFile('Жиры.pdf')\n",
        "\n",
        "iron = drive.CreateFile({'id': '1R4zvF96TQ9HNGBpm6D30JM9vBCkk8IyY'})\n",
        "iron.GetContentFile('Железо.pdf')\n",
        "\n",
        "vitamineC = drive.CreateFile({'id': '15aWQgdxpLvdCT9HS5wTpK_oPKhy7zbOI'})\n",
        "\n",
        "vitamineC.GetContentFile('Витамин C.pdf')\n",
        "\n",
        "file_id = find_file()\n",
        "db, download_db = download_file(file_id)\n",
        "\n",
        "# Обработку опросника необходимо было вынести в отдельную асинхронную функцию\n",
        "async def process_quest(message: types.Message, question_num):\n",
        "  specific_id, db = await get_user_id_in_base(message.from_user.id)\n",
        "\n",
        "  hypothyroidism, insulinresistance, irondeficit = await get_user_data(message.from_user.id, db, specific_id)\n",
        "  if question_num < 33:\n",
        "    if message.text == \"Да\":\n",
        "      if question_num == 2:\n",
        "        irondeficit += 1\n",
        "      if question_num < 9:\n",
        "        hypothyroidism += 1\n",
        "      elif 9 <= question_num < 21:\n",
        "        insulinresistance += 1\n",
        "      elif 22 <= question_num < 34:\n",
        "        irondeficit += 1\n",
        "      question_num = question_num + 1\n",
        "    else:\n",
        "      question_num = question_num + 1\n",
        "  return question_num, hypothyroidism, insulinresistance, irondeficit\n",
        "\n",
        "\n",
        "\n",
        "# @bot.message_handler(content_types=['text', 'audio', 'document', 'animation', 'game', 'photo', 'sticker', 'video', 'video_note', 'voice', 'location', 'contact', 'venue', 'dice', 'new_chat_members', 'left_chat_member', 'new_chat_title',\n",
        "#                   'new_chat_photo', 'delete_chat_photo', 'group_chat_created', 'supergroup_chat_created', 'channel_chat_created', 'migrate_to_chat_id', 'migrate_from_chat_id', 'pinned_message', 'invoice', 'successful_payment',\n",
        "#                   'connected_website', 'poll', 'passport_data', 'proximity_alert_triggered', 'video_chat_scheduled', 'video_chat_started', 'video_chat_ended', 'video_chat_participants_invited', 'web_app_data',\n",
        "#                   'message_auto_delete_timer_changed', 'forum_topic_created', 'forum_topic_closed', 'forum_topic_reopened', 'forum_topic_edited', 'general_forum_topic_hidden', 'general_forum_topic_unhidden', 'write_access_allowed',\n",
        "#                  'user_shared', 'chat_shared', 'story'])\n",
        "\n",
        "@dp.message_handler()\n",
        "async def sample(message):\n",
        "    global download_db, file_id\n",
        "    questions = list(open('questions.txt', encoding='utf-8'))\n",
        "    # Блок с приветствием от бота\n",
        "    if message.content_type == 'text' and (message.text.lower() == 'старт' or message.text.lower() == '/start'):\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        but1 = types.KeyboardButton('✅ Начать')\n",
        "        markup.add(but1)\n",
        "        await bot.send_message(message.chat.id,\n",
        "                         \"Здравствуйте! Я Онлайн нутрицолог, моя задача - помочь Вам\"\n",
        "                         \" с решением возможных проблем со здоровьем путём применения\"\n",
        "                         \" <b>БАДов и витаминов</b>. Приступим к работе?\", parse_mode=\"html\", reply_markup=markup)\n",
        "        await UserStates.start_bot.set()\n",
        "    else await bot.send_message(message.chat.id,\"Пожалуйста, отвечайте корректно. Используйте кнопки, предложенные под полем для ответа.\")\n",
        "\n",
        "\n",
        "\n",
        "@dp.message_handler(state=UserStates.start_bot)\n",
        "async def start_bot(message: types.Message, state: FSMContext):\n",
        "\n",
        "    # Блок с пользовательским соглашением\n",
        "    if message.content_type == 'text' and message.text == '✅ Начать':\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        but2 = types.KeyboardButton('Подтверждаю')\n",
        "        markup.row(but2)\n",
        "        await bot.send_document(message.chat.id, open(r'Пользовательское соглашение.pdf', 'rb'))\n",
        "        await bot.send_document(message.chat.id, open(r'Обработка_персональных_данных.pdf', 'rb'))\n",
        "        await bot.send_message(message.chat.id, 'ℹ️ Прежде чем работать с ботом, <b>обязательно</b>'\n",
        "                                            ' обратите внимание на пользовательское соглашение.'\n",
        "                                            '\\n \\nНажимая кнопку \"Подтверждаю\", Вы автоматически считаетесь '\n",
        "                                            'ознакомленными'\n",
        "                                            ' и согласными с его условиями.', parse_mode='html', reply_markup=markup)\n",
        "\n",
        "\n",
        "    # Блок со службой поддержки\n",
        "    elif message.content_type == 'text' and message.text == 'Подтверждаю':\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        button4 = types.KeyboardButton('✅ Хорошо')\n",
        "        markup.row(button4)\n",
        "        await bot.send_message(message.chat.id, \"ℹ️ Обратите внимание, что у нас есть <b>Служба Поддержки</b>,\"\n",
        "                                            \" в которую Вы можете обратиться в случае возникновения каких-либо жалоб\"\n",
        "                                            \" или вопросов.\"\n",
        "                                            \"\\n<b>Электронная почта Службы Поддержки </b>: lunarfly_off@mail.ru\"\n",
        "                                            \"\\nСреднее время ответа <b>Службы Поддержки </b>занимает от 1 до 3 рабочих дней.\",\n",
        "                          parse_mode=\"html\", reply_markup=markup)\n",
        "        await UserStates.to_questionary.set()\n",
        "    else:\n",
        "        await bot.send_message(message.chat.id,\"Пожалуйста, отвечайте корректно. Используйте кнопки, предложенные под полем для ответа.\")\n",
        "\n",
        "@dp.message_handler(state=UserStates.to_questionary)\n",
        "async def start_questionary(message: types.Message,state: FSMContext):\n",
        "    questions = list(open('questions.txt', encoding='utf-8'))\n",
        "    # !!!Пишите все, что должно быть после соглашения в блоке снизу\n",
        "    if message.content_type == 'text' and (message.text == '✅ Хорошо' or message.text == \"Начать проходить заново\"):\n",
        "      specific_id, db = await get_user_id_in_base(message.from_user.id)\n",
        "      if specific_id != None:\n",
        "        db = db.drop(db.index[specific_id])\n",
        "        db.to_csv(\"user_data.csv\", index=False)\n",
        "      with open('user_data.csv', 'a') as f:\n",
        "        f.write(f\"{message.from_user.id}, {1}, {0}, {0}, {0}, {0}\\n\") # 1 1 0 0 0 -> 1 0 0 0 0\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "      button3 = types.KeyboardButton(\"Приступаем\")\n",
        "      markup.row(button3)\n",
        "      await bot.send_message(message.chat.id, \"Итак, начнем с опросника. В нём будут называться различные симптомы, которые могут быть представлены у Вас.\"\n",
        "                                        \" Отвечайте да, если сталкиваетесь с их проявлением, либо нет, если симптом Вас не беспокоит.\"\n",
        "                                        \" Просим отвечать честно, только так мы сможем дать Вам рекомендации.\",\n",
        "                        reply_markup=markup)\n",
        "\n",
        "\n",
        "    # Начало блока с опросником\n",
        "    elif message.content_type == 'text' and message.text == 'Приступаем':\n",
        "      specific_id, db = await get_user_id_in_base(message.from_user.id)\n",
        "      question_num = list(db['question'])[specific_id]\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "      buttonYes = types.KeyboardButton('Да')\n",
        "      buttonNo = types.KeyboardButton('Нет')\n",
        "      buttonStartAgain = types.KeyboardButton(\"Начать проходить заново\")\n",
        "      markup.row(buttonYes, buttonNo, buttonStartAgain)\n",
        "      await bot.send_message(message.chat.id, f\"{questions[question_num]}\",reply_markup=markup)\n",
        "\n",
        "    # Открываем БД с данными о пользователе\n",
        "    elif message.content_type == 'text' and (message.text == \"Да\" or message.text == \"Нет\"):\n",
        "      specific_id, db = await get_user_id_in_base(message.from_user.id)\n",
        "      question_num = list(db['question'])[specific_id]\n",
        "      question_num, hypothyroidism, insulinresistance, irondeficit = await process_quest(message, question_num)\n",
        "\n",
        "        # На последнем вопросе уходим сюда, и только тогда он его засчитает\n",
        "      if question_num == 33:\n",
        "        db = db.drop(db.index[specific_id])\n",
        "        db.to_csv(\"user_data.csv\", index=False)\n",
        "        with open('user_data.csv', 'a') as f:\n",
        "          f.write(f\"{message.from_user.id}, {1}, {question_num}, {hypothyroidism}, {insulinresistance}, {irondeficit}\\n\")\n",
        "          db = pd.read_csv('user_data.csv')\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        agreeButton = types.KeyboardButton('Узнать результаты')\n",
        "        markup.row(agreeButton)\n",
        "        await bot.send_message(message.chat.id, f\"{questions[question_num]}\",reply_markup=markup)\n",
        "      else:\n",
        "        db = db.drop(db.index[specific_id])\n",
        "        db.to_csv(\"user_data.csv\", index=False)\n",
        "        with open('user_data.csv', 'a') as f:\n",
        "            f.write(f\"{message.from_user.id}, {1}, {question_num}, {hypothyroidism}, {insulinresistance}, {irondeficit}\\n\")\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        buttonYes = types.KeyboardButton('Да')\n",
        "        buttonNo = types.KeyboardButton('Нет')\n",
        "        buttonStartAgain = types.KeyboardButton(\"Начать проходить заново\")\n",
        "        markup.row(buttonYes, buttonNo, buttonStartAgain)\n",
        "        await bot.send_message(message.chat.id, f\"{questions[question_num]}\",reply_markup=markup)\n",
        "\n",
        "    elif message.content_type == 'text' and message.text == \"Узнать результаты\":\n",
        "      specific_id, db = await get_user_id_in_base(message.from_user.id)\n",
        "      hypothyroidism, insulinresistance, irondeficit = await  get_user_data(message.from_user.id, db, specific_id)\n",
        "      await show_results(message, hypothyroidism, insulinresistance, irondeficit, specific_id, db, message.from_user.id)\n",
        "\n",
        "    elif message.text == \"Начать проходить заново\":\n",
        "      await UserStates.to_questionary.set()\n",
        "    else:\n",
        "      await bot.send_message(message.chat.id, \"Пожалуйста, отвечайте корректно. Используйте кнопки, предложенные под полем для ответа.\")\n",
        "\n",
        "@dp.message_handler()\n",
        "async def show_results(message: types.Message, hypothyroidism: int, insulinresistance: int, irondeficit: int, specific_id: int, db, user_id):\n",
        "    if hypothyroidism == irondeficit == insulinresistance == 0:\n",
        "        await bot.send_message(message.chat.id, \"Вы можете сдать следующие анализы(даны в табличке ниже), чтобы быть уверенными, что никаких проблем у Вас нет.\"\n",
        "                                              \"Результаты анализов впишите в данную табличку и отправьте боту\")\n",
        "        await print_instructions(message)\n",
        "        hypo = drive.CreateFile({'id': '1vGqPSv5AExcS3yvR0gC7KQ9_fvHMcS9e'})\n",
        "        hypo.GetContentFile('Анализы3.xlsx')\n",
        "        await bot.send_document(message.chat.id, open('Анализы3.xlsx', 'rb'))\n",
        "        await UserStates.to_analysis.set()\n",
        "      #  dp.message.register(get_analysis, UserState to_analysis)\n",
        "    else:\n",
        "      db = db.drop(db.index[specific_id])\n",
        "      db.to_csv(\"user_data.csv\", index=False)\n",
        "      with open('user_data.csv', 'a') as f:\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard = True, one_time_keyboard = True)\n",
        "        okButton = types.KeyboardButton(\"Понятно\")\n",
        "        markup.row(okButton)\n",
        "        await bot.send_message(message.chat.id, \"У Вас возможны дефициты следующих элементов: \")\n",
        "        if hypothyroidism > irondeficit or hypothyroidism > insulinresistance:\n",
        "            await bot.send_document(message.chat.id, open(r\"Цинк.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Йод.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Селен.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Жиры.pdf\", 'rb'), reply_markup=markup)\n",
        "            f.write(f\"{user_id}, {1}, {MAX_QUESTIONS}, {hypothyroidism}, {insulinresistance}, {irondeficit}, {1}\\n\")\n",
        "            await UserStates.after_analysis.set()\n",
        "        elif irondeficit > insulinresistance or irondeficit >= hypothyroidism:\n",
        "            await bot.send_document(message.chat.id, open(r\"Цинк.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Хром.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Магний.pdf\", 'rb'), reply_markup=markup)\n",
        "            f.write(f\"{user_id}, {1}, {MAX_QUESTIONS}, {hypothyroidism}, {insulinresistance}, {irondeficit}, {2}\\n\")\n",
        "            await UserStates.after_analysis.set()\n",
        "        elif irondeficit == insulinresistance or insulinresistance > irondeficit or insulinresistance >= hypothyroidism:\n",
        "            await bot.send_document(message.chat.id, open(r\"Кобаламин (B12).pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Фолиевая кислота.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Медь.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Витамин C.pdf\", 'rb'))\n",
        "            await bot.send_document(message.chat.id, open(r\"Железо.pdf\", 'rb'), reply_markup = markup)\n",
        "            f.write(f\"{user_id}, {1}, {MAX_QUESTIONS}, {hypothyroidism}, {insulinresistance}, {irondeficit}, {3}\\n\")\n",
        "            await UserStates.after_analysis.set()\n",
        "\n",
        "@dp.message_handler(state=UserStates.after_analysis)\n",
        "async def start_after_analysis(message: types.Message, state: FSMContext):\n",
        "    if message.content_type == 'text' and message.text == \"Понятно\":\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "      continueButton1 = types.KeyboardButton(\"Продолжить\")\n",
        "      markup.row(continueButton1)\n",
        "      await bot.send_message(message.chat.id, \"Если Вы хотите узнать, какой комплекс БАДов Вам необходим,\\n\"\n",
        "                                        \"нужно сдать анализы. Продолжаем?\", reply_markup = markup)\n",
        "\n",
        "\n",
        "    elif message.content_type == 'text' and message.text == 'Продолжить':\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "      continueButton2 = types.KeyboardButton(\"Прочитано\")\n",
        "      markup.row(continueButton2)\n",
        "      await print_instructions(message)\n",
        "      await bot.send_message(message.chat.id, \"Пожалуйста, обращайте внимание на единицы измерений, которые указаны в таблице.\"\n",
        "        \"В случае, когда Вы записываете десятичное число через точку и таблица меняет его на дату, запишите это число\"\n",
        "        \"через запятую. При записи чисел НЕ используйте пробелы(при указании диапазона используйте дефис).\", reply_markup=markup)\n",
        "\n",
        "    # Выдаем соответствующую табличку, куда записывать результаты\n",
        "    elif (message.content_type == 'text' or message.content_type == 'document') and message.text == \"Прочитано\":\n",
        "      specific_id, db = await  get_user_id_in_base(message.from_user.id)\n",
        "      table_num = list(db['table_num'])[specific_id]\n",
        "      await bot.send_message(message.chat.id, \"В данной ниже таблице вместо нулей заполните Ваши результаты анализов.\"\n",
        "                       \"Удостоверьтесь, что Ферритин, ТТГ и инсулин заполнены!\")\n",
        "      if table_num == 1:\n",
        "        hypo = drive.CreateFile({'id': '1vGqPSv5AExcS3yvR0gC7KQ9_fvHMcS9e'})\n",
        "        hypo.GetContentFile('Анализы3.xlsx')\n",
        "        await bot.send_document(message.chat.id, open('Анализы3.xlsx', 'rb'))\n",
        "        await UserStates.to_analysis.set()\n",
        "      elif table_num == 2:\n",
        "        inresist = drive.CreateFile({'id': '1YN07Hl4jUtQj5r07_1ak7xiXkWX1AN22'})\n",
        "        inresist.GetContentFile('Анализы1.xlsx')\n",
        "        await bot.send_document(message.chat.id, open('Анализы1.xlsx', 'rb'))\n",
        "        await UserStates.to_analysis.set()\n",
        "      else:\n",
        "        irondef = drive.CreateFile({'id': '17UXKuhm_gkyF2tixi8SGea7S4Rhsz0mz'})\n",
        "        irondef.GetContentFile('Анализы2.xlsx')\n",
        "        await bot.send_document(message.chat.id, open('Анализы2.xlsx', 'rb'))\n",
        "        await UserStates.to_analysis.set()\n",
        "\n",
        "    else:\n",
        "      await bot.send_message(message.chat.id, \"Пожалуйста, отвечайте корректно. Используйте кнопки, предложенные под полем для ответа.\")\n",
        "# Получаем анализы от пользователя\n",
        "@dp.message_handler(content_types=['document'], state=UserStates.to_analysis)\n",
        "async def get_analysis(message: types.Message, state: FSMContext):\n",
        "  try:\n",
        "    file_name = message.document.file_name\n",
        "    result_table = await bot.get_file(message.document.file_id)\n",
        "    downloaded_file = await bot.download_file(result_table.file_path)\n",
        "    with open(file_name, 'wb') as new_file:\n",
        "      new_file.write(downloaded_file.getvalue())\n",
        "    async with state.proxy() as data:\n",
        "      data['table'] = file_name\n",
        "    markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "    manButton = types.KeyboardButton(\"Мужчина\")\n",
        "    womanButton = types.KeyboardButton(\"Женщина\")\n",
        "    markup.row(manButton, womanButton)\n",
        "    await bot.send_message(message.chat.id, \"Для корректных результатов, необходимо узнать Ваш пол.\", reply_markup = markup)\n",
        "    await UserStates.to_process_analysis.set()#Поменять это и все остальные next_step_handler\n",
        "\n",
        "  except Exception:\n",
        "    await bot.reply_to(message, \"Упс, произошла ошибка, попробуйте еще раз!\")\n",
        "    await UserStates.to_analysis.set()#Поменять это и все остальные next_step_handler\n",
        "\n",
        "@dp.message_handler(state=UserStates.to_process_analysis)\n",
        "async def process_analysis(message: types.Message, state: FSMContext):\n",
        "  print(message.from_user.id)\n",
        "  specific_id, db = await get_user_id_in_base(message.from_user.id)\n",
        "  print(specific_id)\n",
        "  hypothyroidism, insulinresistance, irondeficit = await get_user_data(message.from_user.id, db, specific_id)\n",
        "  db = db.drop(db.index[specific_id])\n",
        "  db.to_csv(\"user_data.csv\", index=False)\n",
        "  async with state.proxy() as data:\n",
        "    file_name=data['table']\n",
        "  results_to_compare = pd.DataFrame(pd.read_excel(file_name))\n",
        "  with open('user_data.csv', 'a') as f:\n",
        "    f.write(f\"{message.from_user.id}, {1}, {MAX_QUESTIONS}, {hypothyroidism}, {insulinresistance}, {irondeficit}, {0}, {message.text}\\n\")\n",
        "    db = pd.read_csv('user_data.csv')\n",
        "  main_analysis = {}\n",
        "  for i in range(results_to_compare.count()[0]):\n",
        "    row = list(results_to_compare.loc[i])\n",
        "    if row[1] != 0 and not(pd.isnull(row[1])):\n",
        "      if row[0] == \"Инсулин\" or row[0] == \"ТТГ\" or row[0] == \"Ферритин\":\n",
        "        if row[1].is_integer() or isinstance(row[1], np.floating):\n",
        "          main_analysis[row[0]] = row[1]\n",
        "\n",
        "\n",
        "  # Саша, здесь есть повторяющиеся строки. Если у тебя есть идея, как от них избавиться, то давай :))) /// в ф-ию var_sender вынес все эти операции (она выше в комменнтариях).\n",
        "  if (\"Инсулин\" not in main_analysis) or (\"Ферритин\" not in main_analysis) or (\"ТТГ\" not in main_analysis):\n",
        "    await bot.send_message(message.chat.id, \"Вы ввели некорректные данные в таблице, попробуйте еще раз!\"\n",
        "                     \"Удостоверьтесь, что Ферритин, ТТГ и инсулин заполнены!\")\n",
        "    await UserStates.to_analysis.set()\n",
        "  elif 50 <= main_analysis[\"Ферритин\"] <= 150 and 2 <= main_analysis[\"Инсулин\"] <= 6 and 0.4 <= float(main_analysis[\"ТТГ\"]) <= 2.0:\n",
        "    fourthvar = drive.CreateFile({'id': '1qmJapfl1Cx2G6RTAzEvCgCz047gEr8Dr'})\n",
        "    fourthvar.GetContentFile('Вариант4.pdf')\n",
        "    await bot.send_document(message.chat.id, open(r\"Вариант4.pdf\", 'rb'))\n",
        "    await bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "    await UserStates.to_AI.set()\n",
        "  else:\n",
        "    if main_analysis[\"Ферритин\"] <= main_analysis[\"Инсулин\"] or main_analysis[\"Инсулин\"] >= main_analysis[\"ТТГ\"]:\n",
        "      firstvar = drive.CreateFile({'id': '1t1yqo_6RSeE7Jp1DB11sd_2cCTsTj9_I'})    # замена на var_sender(message.chat.id, 1)\n",
        "      firstvar.GetContentFile('Вариант1.pdf')\n",
        "      await bot.send_document(message.chat.id, open(r\"Вариант1.pdf\", 'rb'))\n",
        "      await bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "      await UserStates.to_AI.set()\n",
        "    elif  main_analysis[\"Ферритин\"] >= main_analysis[\"ТТГ\"] or main_analysis[\"Ферритин\"] > main_analysis[\"Инсулин\"]:\n",
        "      secondvar = drive.CreateFile({'id': '15ULzx1xnjvafKmuR0wsLeQwHxAteVn9Z'})    # замена на var_sender(message.chat.id, 2)\n",
        "      secondvar.GetContentFile('Вариант2.pdf')\n",
        "      await bot.send_document(message.chat.id, open(r\"Вариант2.pdf\", 'rb'))\n",
        "      await bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "      await UserStates.to_AI.set()\n",
        "    else:\n",
        "      thirdvar = drive.CreateFile({'id': '1mkr7plbpyImrEtVrgaZtuwebi_xKFxrU'})    # замена на var_sender(message.chat.id, 3)\n",
        "      thirdvar.GetContentFile('Вариант3.pdf')\n",
        "      await bot.send_document(message.chat.id, open(r\"Вариант3.pdf\", 'rb'))\n",
        "      await bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "      await UserStates.to_AI.set()\n",
        "\n",
        "@dp.message_handler(state=UserStates.to_AI)\n",
        "async def ask_model(user_prompt: str, state: FSMContext):\n",
        "  message = user_prompt\n",
        "  if user_prompt.text.lower() != 'стоп':\n",
        "    user_prompt = translator.translate(user_prompt.text).text\n",
        "    output = pipe(f'''[INST] <<SYS>>\n",
        "    {system_prompt}<</SYS>>\n",
        "    {user_prompt}[/INST]''', return_full_text = False)[0]['generated_text']\n",
        "    translated_output = translator.translate(output, src=\"en\", dest=\"ru\")\n",
        "    await bot.send_message(message.chat.id, translated_output.text)\n",
        "    await UserStates.to_AI.set()\n",
        "  else:\n",
        "    againButton = types.KeyboardButton(\"Начать проходить заново\")\n",
        "    markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "    markup.row(againButton)\n",
        "    await bot.send_message(message.chat.id, \"Вы можете начать проходить бота заново.\", reply_markup = markup)\n",
        "    await UserStates.to_questionary.set()\n",
        "\n",
        "\n",
        "\n",
        "# Здесь запускается бот\n",
        "async def main():\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1RAvpjkMV_-",
        "outputId": "82bf203c-c08c-4547-d17a-95909661dce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiogram==2.23.1 in /usr/local/lib/python3.10/dist-packages (2.23.1)\n",
            "Requirement already satisfied: aiohttp<3.9.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from aiogram==2.23.1) (3.8.6)\n",
            "Requirement already satisfied: Babel<2.10.0,>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from aiogram==2.23.1) (2.9.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from aiogram==2.23.1) (2023.11.17)\n",
            "Requirement already satisfied: magic-filter>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from aiogram==2.23.1) (1.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.10/dist-packages (from Babel<2.10.0,>=2.9.1->aiogram==2.23.1) (2023.3.post1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<3.9.0,>=3.8.0->aiogram==2.23.1) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install aiogram==2.23.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qttgaUKt7WZ",
        "outputId": "22957436-e08a-4dee-9ee5-acd20a91c196"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.5.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install telebot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVlLDKB0jSXW",
        "outputId": "db69792e-d159-4079-f6b2-d4571feb54e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: telebot in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
            "Requirement already satisfied: pyTelegramBotAPI in /usr/local/lib/python3.10/dist-packages (from telebot) (4.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from telebot) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WT_DDVGqHnC"
      },
      "source": [
        " **НЕЙРОНКА**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a7ovt-mUntgh"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install accelerate bitsandbytes transformers optimum googletrans==3.1.0a0\n",
        "!pip install -qqq auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "Gy0uwJp_43ZI",
        "outputId": "0980554e-531c-4d63-fdb4-007596bdbbb7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b9bf170f5208>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m from transformers import (\n\u001b[1;32m      4\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.7.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mAutoPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# We want the exported object to be the class, so we first import the module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# to make sure a later import doesn't overwrite the class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_deprecated_config\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Deprecations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_guard_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_client\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlapack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mducc_fft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mducc_fft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jaxlib/lapack.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mir\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstablehlo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhlo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jaxlib/mlir/ir.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_mlir_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mir\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_mlir_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mir\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_GlobalDebug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_mlir_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mlir\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_type_caster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jaxlib/mlir/_mlir_libs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0m_site_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jaxlib/mlir/_mlir_libs/__init__.py\u001b[0m in \u001b[0;36m_site_initialize\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_mlir\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: initialization failed",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    GPTQConfig\n",
        ")\n",
        "from google.colab import drive\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "drive.mount('/content/drive')\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "9dc5e9f2dcee4a21b208462946e4c2cd",
            "12c309b6981c46b699bf8029e7dd3887",
            "33675f8dafaa44768562bcef2c456485",
            "e8575e253a2d41718ce8879840e4fcc4",
            "73aeed0db15b48cfb2678415524978bd",
            "66dee0cdbd12452699845be77fdb55f5",
            "59e18ce1db274f859ddec213e92a1ed2",
            "96e695b198b64993a3a9568da05cd8d3",
            "26e9501a5d1d483ca43a24a302f0854d",
            "a09f218ef7594790bc59de3dd6e56833",
            "996ce67990694b9d8d4f9270bcd2386d",
            "9594d9c4ab11489e844291430cb21cc2",
            "7378b568c7fc435e8ace7fb29b9ea601",
            "a25452d9769144e6831bf3cec12668ea",
            "1c4980ad542543f89afbe24c0b44df54",
            "cc39c5a2bf784566938e7883a0b488a7",
            "9764bef87d38498cbfaa6c7e8027acea",
            "2a082736c36a43d8b34df6bb60618909",
            "77b78f3fe4674a95b1089b1ebc17f796",
            "b9fb2917cddb4866a6280e72d60f4c76",
            "13de789a2999468da38cf4e867e58289",
            "b431b61e8b00441eaf2c35332242f295"
          ]
        },
        "id": "H7e1zXvYoYAh",
        "outputId": "4edcb2eb-610b-4044-ec8a-16f39a6d37d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dc5e9f2dcee4a21b208462946e4c2cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9594d9c4ab11489e844291430cb21cc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-91fdd07e5a0d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0madapter_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/checkpoint-240'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(base_model_name,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                              \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                              quantization_config=GPTQConfig(bits=4,disable_exllama=True))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3478\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3480\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3481\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3482\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3868\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3869\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_fsdp_enabled_and_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3870\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   3871\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3872\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;31m# For backward compatibility with older versions of `accelerate`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SCB\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "### Инициализация модели ######################################################################################\n",
        "# Эта ячейка загружает модель в видеокарту, ни в коем случае ее нельзя запускать несколько раз за одну сессию,\n",
        "# иначе в видюху загрузится несколько моделей и в ней начнет закачниватся видеопамять,\n",
        "# если все таки нужно повторно запустить код, то сначала перезапустите среду чтобы освободить\n",
        "# память от прошлой модели.\n",
        "# По умолчанию модель занимает не более 10 ГБ VRAM, если занято больше значит вы что то делаете не так\n",
        "#####################################################################################################################\n",
        "\n",
        "base_model_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "adapter_model = '/content/drive/MyDrive/checkpoint-240'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
        "                                             device_map={\"\": 0},\n",
        "                                             quantization_config=GPTQConfig(bits=4,disable_exllama=True))\n",
        "model = PeftModel.from_pretrained(model, adapter_model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0XLZBV9kKfe",
        "outputId": "9a55b7b7-9156-4ef5-e5a3-7a0712799d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "### Параметры модели ###\n",
        "\n",
        "pipe = pipeline('text-generation',model=model,tokenizer=tokenizer,\n",
        "                max_new_tokens=300, # Максимальная длина ответа, сильное снижение может привести к обрезанию ответов\n",
        "                do_sample=True,   # не трогать\n",
        "                temperature=0.1,  # рандомность ответа, чем больше это число тем более неожиданные и креативные ответы будет давать модель, если число маленькое то ответы будут однообразными но уверенными.\n",
        "                top_p=0.95, #  тоже рандомность, чем меньше тем больше модель будет отходить от контекста.\n",
        "                top_k=40, # аналагично параметру выше, не рекомендуется менять.\n",
        "                repetition_penalty=1.15  # штраф за повторение, чем выше тем меньше модель будет повторять одни и те же слова/предложения, слишком высокое значение может привести к бреду/галлюцинациям\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjQ735UfqU_U"
      },
      "outputs": [],
      "source": [
        "### Системный промпт, в нем вы обьясняете боту кто он по жизни и в чем его цель, что ему стоит делать а что нет, можно свободно изменять.\n",
        "system_prompt = \"You are a helpful, respectful and honest assistant, you answer questions regarding food supplements and vitamins. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Answer the topic of the question, do not deviate from the topic of the question, do not philosophize, be precise and concise, use less than 150 words. Don't use any diagnoses or statistics in your answer. Use as few medical terms as possible. Do not ask questions back. Do not advise any vitamins.\"\n",
        "\n",
        "def ask_model(user_prompt: str):\n",
        "  user_prompt = translator.translate(user_prompt).text\n",
        "  output = pipe(f'''[INST] <<SYS>>\n",
        "  {system_prompt}<</SYS>>\n",
        "  {user_prompt}[/INST]''', return_full_text = False)[0]\n",
        "  translated_output = translator.translate(output, src=\"en\", dest=\"ru\")\n",
        "  return translated_output.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAT9bun0wgu1",
        "outputId": "86d12296-530c-4819-a91e-aa8cb4a1b49e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'generated_text': 'Я не могу назначать лекарства или рекомендовать дозировки. Однако если эта проблема возникает у вас каждый день, необходимо обращаться к специалисту (отоларингологу, аллергологу, проконсультироваться у этих врачей). Причину чихания может устранить врач. Во время такого наблюдения важно соблюдать правила гигиены: избегать контактов с людьми, больными ОРВИ, регулярно мыть руки, носить маску, защищаться от загрязнителей окружающей среды – здесь часто прячутся бактерии и вирусы. Если чихание возникает периодически, желательно принять Иммуцет плюс (продается в аптеке). Этот препарат основан на гомеопатии, поэтому не рекомендуется использовать его, если вы являетесь последователем индуизма или буддизма. Кроме того, прежде чем вынимать, проконсультируйтесь со специалистом. Если чихание сопровождается покраснением слизистой носа или гнойными выделениями, необходимо срочно обратиться за помощью к отоларингологу. При чихании необходимо соблюдать правила гигиены, избегать самолечения. Вы не можете решить свою проблему? Тогда обратитесь к специалисту! Вот несколько советов при чихании: • старайтесь держать голову в тепле, • используйте увлажнитель воздуха в квартире, • используйте смазку с соленой водой'}\n"
          ]
        }
      ],
      "source": [
        "print(ask_model('Как лечить хроническое чихание?'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMV/DiLJoaAY1Z2kR1DiEzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12c309b6981c46b699bf8029e7dd3887": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66dee0cdbd12452699845be77fdb55f5",
            "placeholder": "​",
            "style": "IPY_MODEL_59e18ce1db274f859ddec213e92a1ed2",
            "value": "config.json: 100%"
          }
        },
        "13de789a2999468da38cf4e867e58289": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4980ad542543f89afbe24c0b44df54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13de789a2999468da38cf4e867e58289",
            "placeholder": "​",
            "style": "IPY_MODEL_b431b61e8b00441eaf2c35332242f295",
            "value": " 7.26G/7.26G [01:10&lt;00:00, 142MB/s]"
          }
        },
        "26e9501a5d1d483ca43a24a302f0854d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a082736c36a43d8b34df6bb60618909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33675f8dafaa44768562bcef2c456485": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e695b198b64993a3a9568da05cd8d3",
            "max": 837,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26e9501a5d1d483ca43a24a302f0854d",
            "value": 837
          }
        },
        "59e18ce1db274f859ddec213e92a1ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66dee0cdbd12452699845be77fdb55f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7378b568c7fc435e8ace7fb29b9ea601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9764bef87d38498cbfaa6c7e8027acea",
            "placeholder": "​",
            "style": "IPY_MODEL_2a082736c36a43d8b34df6bb60618909",
            "value": "model.safetensors: 100%"
          }
        },
        "73aeed0db15b48cfb2678415524978bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b78f3fe4674a95b1089b1ebc17f796": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9594d9c4ab11489e844291430cb21cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7378b568c7fc435e8ace7fb29b9ea601",
              "IPY_MODEL_a25452d9769144e6831bf3cec12668ea",
              "IPY_MODEL_1c4980ad542543f89afbe24c0b44df54"
            ],
            "layout": "IPY_MODEL_cc39c5a2bf784566938e7883a0b488a7"
          }
        },
        "96e695b198b64993a3a9568da05cd8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9764bef87d38498cbfaa6c7e8027acea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996ce67990694b9d8d4f9270bcd2386d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc5e9f2dcee4a21b208462946e4c2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12c309b6981c46b699bf8029e7dd3887",
              "IPY_MODEL_33675f8dafaa44768562bcef2c456485",
              "IPY_MODEL_e8575e253a2d41718ce8879840e4fcc4"
            ],
            "layout": "IPY_MODEL_73aeed0db15b48cfb2678415524978bd"
          }
        },
        "a09f218ef7594790bc59de3dd6e56833": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a25452d9769144e6831bf3cec12668ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b78f3fe4674a95b1089b1ebc17f796",
            "max": 7259449480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9fb2917cddb4866a6280e72d60f4c76",
            "value": 7259449480
          }
        },
        "b431b61e8b00441eaf2c35332242f295": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9fb2917cddb4866a6280e72d60f4c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc39c5a2bf784566938e7883a0b488a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8575e253a2d41718ce8879840e4fcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a09f218ef7594790bc59de3dd6e56833",
            "placeholder": "​",
            "style": "IPY_MODEL_996ce67990694b9d8d4f9270bcd2386d",
            "value": " 837/837 [00:00&lt;00:00, 29.0kB/s]"
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}