{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpv5xDKcN3X0",
        "outputId": "1ca5df40-b26b-4327-a9db-6d12915b74c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
            "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            "2023-12-10 13:42:43,269 (__init__.py:960 MainThread) ERROR - TeleBot: \"Infinity polling exception: list indices must be integers or slices, not NoneType\"\n",
            "ERROR:TeleBot:Infinity polling exception: list indices must be integers or slices, not NoneType\n",
            "2023-12-10 13:42:43,273 (__init__.py:962 MainThread) ERROR - TeleBot: \"Exception traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 955, in infinity_polling\n",
            "    self.polling(non_stop=True, timeout=timeout, long_polling_timeout=long_polling_timeout,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 1043, in polling\n",
            "    self.__threaded_polling(non_stop=non_stop, interval=interval, timeout=timeout, long_polling_timeout=long_polling_timeout,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 1118, in __threaded_polling\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 1074, in __threaded_polling\n",
            "    self.worker_pool.raise_exceptions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/util.py\", line 147, in raise_exceptions\n",
            "    raise self.exception_info\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/util.py\", line 90, in run\n",
            "    task(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 6801, in _run_middlewares_and_handler\n",
            "    result = handler['function'](message)\n",
            "  File \"<ipython-input-1-6df70baaa60e>\", line 292, in sample\n",
            "    question_num = list(db['question'])[specific_id]\n",
            "TypeError: list indices must be integers or slices, not NoneType\n",
            "\"\n",
            "ERROR:TeleBot:Exception traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 955, in infinity_polling\n",
            "    self.polling(non_stop=True, timeout=timeout, long_polling_timeout=long_polling_timeout,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 1043, in polling\n",
            "    self.__threaded_polling(non_stop=non_stop, interval=interval, timeout=timeout, long_polling_timeout=long_polling_timeout,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 1118, in __threaded_polling\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 1074, in __threaded_polling\n",
            "    self.worker_pool.raise_exceptions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/util.py\", line 147, in raise_exceptions\n",
            "    raise self.exception_info\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/util.py\", line 90, in run\n",
            "    task(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/telebot/__init__.py\", line 6801, in _run_middlewares_and_handler\n",
            "    result = handler['function'](message)\n",
            "  File \"<ipython-input-1-6df70baaa60e>\", line 292, in sample\n",
            "    question_num = list(db['question'])[specific_id]\n",
            "TypeError: list indices must be integers or slices, not NoneType\n",
            "\n",
            "2023-12-10 13:48:36,304 (__init__.py:966 MainThread) ERROR - TeleBot: \"Infinity polling: polling exited\"\n",
            "ERROR:TeleBot:Infinity polling: polling exited\n",
            "2023-12-10 13:48:36,307 (__init__.py:968 MainThread) ERROR - TeleBot: \"Break infinity polling\"\n",
            "ERROR:TeleBot:Break infinity polling\n"
          ]
        }
      ],
      "source": [
        "import telebot\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from telebot import types\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import (\n",
        "     AutoModelForCausalLM,\n",
        "     AutoTokenizer,\n",
        "     pipeline,\n",
        "     GPTQConfig\n",
        " )\n",
        "from google.colab import drive\n",
        "from googletrans import Translator\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# начало нейронки\n",
        "\n",
        "translator = Translator()\n",
        "torch.cuda.is_available()\n",
        "\n",
        "### Инициализация модели ######################################################################################\n",
        "# Эта ячейка загружает модель в видеокарту, ни в коем случае ее нельзя запускать несколько раз за одну сессию,\n",
        "# иначе в видюху загрузится несколько моделей и в ней начнет закачниватся видеопамять,\n",
        "# если все таки нужно повторно запустить код, то сначала перезапустите среду чтобы освободить\n",
        "# память от прошлой модели.\n",
        "# По умолчанию модель занимает не более 10 ГБ VRAM, если занято больше значит вы что то делаете не так\n",
        "#####################################################################################################################\n",
        "\n",
        "base_model_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "adapter_model = '/content/drive/MyDrive/checkpoint-240'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
        "                                              device_map={\"\": 0},\n",
        "                                              quantization_config=GPTQConfig(bits=4,disable_exllama=True))\n",
        "model = PeftModel.from_pretrained(model, adapter_model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "### Параметры модели ###\n",
        "\n",
        "pipe = pipeline('text-generation',model=model,tokenizer=tokenizer,\n",
        "                 max_new_tokens=300, # Максимальная длина ответа, сильное снижение может привести к обрезанию ответов\n",
        "                 do_sample=True,   # не трогать\n",
        "                 temperature=0.1,  # рандомность ответа, чем больше это число тем более неожиданные и креативные ответы будет давать модель, если число маленькое то ответы будут однообразными но уверенными.\n",
        "                 top_p=0.95, #  тоже рандомность, чем меньше тем больше модель будет отходить от контекста.\n",
        "                 top_k=40, # аналагично параметру выше, не рекомендуется менять.\n",
        "                 repetition_penalty=1.15  # штраф за повторение, чем выше тем меньше модель будет повторять одни и те же слова/предложения, слишком высокое значение может привести к бреду/галлюцинациям\n",
        "                )\n",
        "\n",
        "# # ### Системный промпт, в нем вы обьясняете боту кто он по жизни и в чем его цель, что ему стоит делать а что нет, можно свободно изменять.\n",
        "system_prompt = \"You are a helpful, respectful and honest assistant, you answer questions regarding food supplements and vitamins. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Answer the topic of the question, do not deviate from the topic of the question, do not philosophize, be precise and concise, use less than 150 words. Don't use any diagnoses or statistics in your answer. Use as few medical terms as possible. Do not ask questions back. Do not advise any vitamins.\"\n",
        "\n",
        "\n",
        "\n",
        "#конец нейронки\n",
        "\n",
        "# Ищем id файла на диске\n",
        "def find_file():\n",
        "  file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "  for file in file_list:\n",
        "    if file['title'] == 'user_data.csv':\n",
        "        id = file['id']\n",
        "  return id\n",
        "\n",
        "\n",
        "# Скачиваем файл с этим id\n",
        "def download_file(id):\n",
        "  download_db = drive.CreateFile({'id': f'{id}'})\n",
        "  download_db.GetContentFile('user_data.csv')\n",
        "  db = pd.read_csv('user_data.csv', encoding='utf-8', sep=',')\n",
        "  return db, download_db\n",
        "\n",
        "\n",
        "# загружаем файл на диск\n",
        "def upload_file():\n",
        "  file_to_upload = drive.CreateFile({'title': 'user_data.csv'})\n",
        "  file_to_upload.SetContentFile('user_data.csv')\n",
        "  file_to_upload.Upload()\n",
        "\n",
        "\n",
        "# Берем результаты опросника и на основе них выводим возможные дефициты:\n",
        "def show_results(message, hypothyroidism, insulinresistance, irondeficit, specific_id, db, user_id):\n",
        "    if hypothyroidism == irondeficit == insulinresistance == 0:\n",
        "        bot.send_message(message.chat.id, \"Вы можете сдать следующие анализы(даны в табличке ниже), чтобы быть уверенными, что никаких проблем у Вас нет.\"\n",
        "                                              \"Результаты анализов впишите в данную табличку и отправьте боту\")\n",
        "        print_instructions(message)\n",
        "        hypo = drive.CreateFile({'id': '1vGqPSv5AExcS3yvR0gC7KQ9_fvHMcS9e'})\n",
        "        hypo.GetContentFile('Анализы3.xlsx')\n",
        "        bot.send_document(message.chat.id, open('Анализы3.xlsx', 'rb'))\n",
        "        bot.register_next_step_handler(message, get_analysis)\n",
        "    else:\n",
        "      db = db.drop(db.index[specific_id])\n",
        "      db.to_csv(\"user_data.csv\", index=False)\n",
        "      with open('user_data.csv', 'a') as f:\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard = True, one_time_keyboard = True)\n",
        "        okButton = types.KeyboardButton(\"Понятно\")\n",
        "        markup.row(okButton)\n",
        "        bot.send_message(message.chat.id, \"У Вас возможны дефициты следующих элементов: \")\n",
        "        if hypothyroidism > irondeficit or hypothyroidism > insulinresistance:\n",
        "            bot.send_document(message.chat.id, open(r\"Цинк.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Йод.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Селен.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Жиры.pdf\", 'rb'), reply_markup=markup)\n",
        "            f.write(f\"{user_id}, {1}, {33}, {hypothyroidism}, {insulinresistance}, {irondeficit}, {1}\\n\")\n",
        "        elif irondeficit > insulinresistance or irondeficit >= hypothyroidism:\n",
        "            bot.send_document(message.chat.id, open(r\"Цинк.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Хром.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Магний.pdf\", 'rb'), reply_markup=markup)\n",
        "            f.write(f\"{user_id}, {1}, {33}, {hypothyroidism}, {insulinresistance}, {irondeficit}, {2}\\n\")\n",
        "        elif irondeficit == insulinresistance or insulinresistance > irondeficit or insulinresistance >= hypothyroidism:\n",
        "            bot.send_document(message.chat.id, open(r\"Кобаламин (B12).pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Фолиевая кислота.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Медь.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Витамин C.pdf\", 'rb'))\n",
        "            bot.send_document(message.chat.id, open(r\"Железо.pdf\", 'rb'), reply_markup = markup)\n",
        "            f.write(f\"{user_id}, {1}, {33}, {hypothyroidism}, {insulinresistance}, {irondeficit}, {3}\\n\")\n",
        "\n",
        "\n",
        "# Получаем информацию о результатах опроса конкретного пользователя\n",
        "def get_data(user_id, db, specific_id):\n",
        "  hypothyroidism = list(db['Г'])[specific_id]\n",
        "  insulinresistance = list(db['ИР'])[specific_id]\n",
        "  irondeficit = list(db['ЖД'])[specific_id]\n",
        "  return hypothyroidism,insulinresistance,irondeficit\n",
        "\n",
        "\n",
        "# Получаем id пользователя в базе пользователей\n",
        "def get_user_id_in_base(user_id):\n",
        "  with open('user_data.csv') as f:\n",
        "    db = pd.read_csv('user_data.csv')\n",
        "    user_ids = list(db['user_id'])\n",
        "    if user_id in user_ids:\n",
        "      specific_id = user_ids.index(user_id)\n",
        "    else:\n",
        "      specific_id = None\n",
        "  return specific_id, db\n",
        "\n",
        "def print_instructions(message, markup=None):\n",
        "  bot.send_message(message.chat.id, \"Подготовка к анализам:\\n\"\n",
        "                      \"Общие правила подготовки и сдачи анализов крови на витамины и минералы:\\n\"\n",
        "                      \"●прийти в лабораторию заранее, чтобы привести в порядок эмоциональное и физическое состояние\\n\"\n",
        "                      \"●с момента последнего приема пищи должно пройти не менее восьми часов\\n\"\n",
        "                      \"●с утра разрешается пить воду, только чистую без добавок\\n\"\n",
        "                      \"●за неделю до сдачи (примерно) отказаться от употребления содержащих спирт напитков (в Т.Ч.\\n\"\n",
        "                      \"аптечной продукции)\\n\"\n",
        "                      \"●в день сдачи крови желательно воздерживаться от курения\\n\"\n",
        "                      \"●нельзя совмещать дни посещения лаборатории и физиотерапевтических (аппаратных) процедур исключить интенсивные физические нагрузки\\n\"\n",
        "                      \"Если Вы находитесь в состоянии сильных эмоциональных переживаний, возможно получение искаженных результатов.\")\n",
        "\n",
        "\n",
        "# Подгружаем все необходимые файлы\n",
        "user_agreement = drive.CreateFile({'id': '1zPrp6ou173pwdThjG4Uq_Q9087l9pABp'})  # Его id на гугл диске\n",
        "user_agreement.GetContentFile('Пользовательское соглашение.pdf')  # название\n",
        "\n",
        "process_data = drive.CreateFile({'id': '1SYV6HZ344MOZp7gnu6OWAySjIzXexi_9'})\n",
        "process_data.GetContentFile('Обработка_персональных_данных.pdf')\n",
        "\n",
        "questions = drive.CreateFile({'id': '1bKuEAn1piaz6HhLK2QfeVX75zGg6CqC5'})\n",
        "questions.GetContentFile('questions.txt')\n",
        "\n",
        "zinc = drive.CreateFile({'id': '1xGnABMJRE27t3syyG1DaIx7k3XCgXXIp'})\n",
        "zinc.GetContentFile('Цинк.pdf')\n",
        "\n",
        "chrome = drive.CreateFile({'id': '1wyWfmYDdoTZwD0ZwCAWKY74Q8CNVMBgY'})\n",
        "chrome.GetContentFile('Хром.pdf')\n",
        "\n",
        "acid = drive.CreateFile({'id': '1g80MGqMXA2PmAe4vxjchqGe09dMy3pjJ'})\n",
        "acid.GetContentFile('Фолиевая кислота.pdf')\n",
        "\n",
        "test_results = drive.CreateFile({'id': '1x1UoV6Zmn6Ch0P4wXveZ3i-kGv_h_h0k'})\n",
        "test_results.GetContentFile(\"Показания.xlsx\")\n",
        "\n",
        "selen = drive.CreateFile({'id': '1HziBuXzpdVaFr8qWLbeyLjgEDw_X72bj'})\n",
        "selen.GetContentFile('Селен.pdf')\n",
        "\n",
        "copper = drive.CreateFile({'id': '1O1E_Y2lkjXixCo3lfemfbSGchzEAVsEv'})\n",
        "copper.GetContentFile('Медь.pdf')\n",
        "\n",
        "magnium = drive.CreateFile({'id': '18t-tjAuHyqX5PcrMXMlSfvDkIWlRgYjp'})\n",
        "magnium.GetContentFile('Магний.pdf')\n",
        "\n",
        "b12 = drive.CreateFile({'id': '1vb4_lCF5Nja10i3nJzlpUfPuC5D5I5Du'})\n",
        "b12.GetContentFile('Кобаламин (B12).pdf')\n",
        "\n",
        "iodine = drive.CreateFile({'id': '1xK2AvcWMfcEEKThA9j5i7pVdQ2y3RUwl'})\n",
        "iodine.GetContentFile('Йод.pdf')\n",
        "\n",
        "fats = drive.CreateFile({'id': '1UPweqKiIN6qeItcxM4swvRcpaZM8ywXI'})\n",
        "fats.GetContentFile('Жиры.pdf')\n",
        "\n",
        "iron = drive.CreateFile({'id': '1R4zvF96TQ9HNGBpm6D30JM9vBCkk8IyY'})\n",
        "iron.GetContentFile('Железо.pdf')\n",
        "\n",
        "vitamineC = drive.CreateFile({'id': '15aWQgdxpLvdCT9HS5wTpK_oPKhy7zbOI'})\n",
        "\n",
        "vitamineC.GetContentFile('Витамин C.pdf')\n",
        "\n",
        "file_id = find_file()\n",
        "db, download_db = download_file(file_id)\n",
        "\n",
        "\n",
        "token = '6330602631:AAGD-y1wboKQSXOkyJUWlV7UVXmNRVgPN90'\n",
        "bot = telebot.TeleBot(token)\n",
        "@bot.message_handler(content_types=['text', 'audio', 'document', 'animation', 'game', 'photo', 'sticker', 'video', 'video_note', 'voice', 'location', 'contact', 'venue', 'dice', 'new_chat_members', 'left_chat_member', 'new_chat_title',\n",
        "                  'new_chat_photo', 'delete_chat_photo', 'group_chat_created', 'supergroup_chat_created', 'channel_chat_created', 'migrate_to_chat_id', 'migrate_from_chat_id', 'pinned_message', 'invoice', 'successful_payment',\n",
        "                  'connected_website', 'poll', 'passport_data', 'proximity_alert_triggered', 'video_chat_scheduled', 'video_chat_started', 'video_chat_ended', 'video_chat_participants_invited', 'web_app_data',\n",
        "                  'message_auto_delete_timer_changed', 'forum_topic_created', 'forum_topic_closed', 'forum_topic_reopened', 'forum_topic_edited', 'general_forum_topic_hidden', 'general_forum_topic_unhidden', 'write_access_allowed',\n",
        "                 'user_shared', 'chat_shared', 'story'])\n",
        "def sample(message):\n",
        "    global download_db, file_id\n",
        "    user_id = message.from_user.id  # Получаем тг id пользователя\n",
        "    questions = list(open('questions.txt', encoding='utf-8'))\n",
        "    specific_id, db = get_user_id_in_base(message.from_user.id)\n",
        "    # Блок с приветствием от бота\n",
        "    if message.content_type == 'text' and (message.text.lower() == 'старт' or message.text.lower() == '/start') and specific_id == None:\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        but1 = types.KeyboardButton('✅ Начать')\n",
        "        markup.add(but1)\n",
        "        bot.send_message(message.chat.id,\n",
        "                         \"Здравствуйте! Я Онлайн нутрицолог, моя задача - помочь Вам\"\n",
        "                         \" с решением возможных проблем со здоровьем путём применения\"\n",
        "                         \" <b>БАДов и витаминов</b>. Приступим к работе?\", parse_mode=\"html\", reply_markup=markup)\n",
        "\n",
        "    # Блок с пользовательским соглашением\n",
        "    elif message.content_type == 'text' and message.text == '✅ Начать' and specific_id == None:\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        but2 = types.KeyboardButton('Подтверждаю')\n",
        "        markup.row(but2)\n",
        "        bot.send_document(message.chat.id, open(r'Пользовательское соглашение.pdf', 'rb'))\n",
        "        bot.send_document(message.chat.id, open(r'Обработка_персональных_данных.pdf', 'rb'))\n",
        "        bot.send_message(message.chat.id, 'ℹ️ Прежде чем работать с ботом, <b>обязательно</b>'\n",
        "                                            ' обратите внимание на пользовательское соглашение.'\n",
        "                                            '\\n \\nНажимая кнопку \"Подтверждаю\", Вы автоматически считаетесь '\n",
        "                                            'ознакомленными'\n",
        "                                            ' и согласными с его условиями.', parse_mode='html', reply_markup=markup)\n",
        "\n",
        "\n",
        "    # Блок со службой поддержки\n",
        "    elif message.content_type == 'text' and message.text == 'Подтверждаю' and specific_id == None:\n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "        button4 = types.KeyboardButton('✅ Хорошо')\n",
        "        markup.row(button4)\n",
        "        bot.send_message(message.chat.id, \"ℹ️ Обратите внимание, что у нас есть <b>Служба Поддержки</b>,\"\n",
        "                                            \" в которую Вы можете обратиться в случае возникновения каких-либо жалоб\"\n",
        "                                            \" или вопросов.\"\n",
        "                                            \"\\n<b>Электронная почта Службы Поддержки </b>: lunarfly_off@mail.ru\"\n",
        "                                            \"\\nСреднее время ответа <b>Службы Поддержки </b>занимает от 1 до 3 рабочих дней.\",\n",
        "                          parse_mode=\"html\", reply_markup=markup)\n",
        "\n",
        "    # !!!Пишите все, что должно быть после соглашения в блоке снизу\n",
        "    elif message.content_type == 'text' and (message.text == '✅ Хорошо' or message.text == \"Начать проходить заново\"):\n",
        "      specific_id, db = get_user_id_in_base(message.from_user.id)\n",
        "      if specific_id != None:\n",
        "        db = db.drop(db.index[specific_id])\n",
        "        db.to_csv(\"user_data.csv\", index=False)\n",
        "      with open('user_data.csv', 'a') as f:\n",
        "        f.write(f\"{user_id}, {1}, {0}, {0}, {0}, {0}\\n\") # 1 1 0 0 0 -> 1 0 0 0 0\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "      button3 = types.KeyboardButton(\"Приступаем\")\n",
        "      markup.row(button3)\n",
        "      bot.send_message(message.chat.id, \"Итак, начнем с опросника. В нём будут называться различные симптомы, которые могут быть представлены у Вас.\"\n",
        "                                        \" Отвечайте да, если сталкиваетесь с их проявлением, либо нет, если симптом Вас не беспокоит.\"\n",
        "                                        \" Просим отвечать честно, только так мы сможем дать Вам рекомендации.\",\n",
        "                        reply_markup=markup)\n",
        "\n",
        "\n",
        "    # Начало блока с опросником\n",
        "    elif message.content_type == 'text' and message.text == 'Приступаем':\n",
        "      specific_id, db = get_user_id_in_base(user_id)\n",
        "      question_num = list(db['question'])[specific_id]\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "      buttonYes = types.KeyboardButton('Да')\n",
        "      buttonNo = types.KeyboardButton('Нет')\n",
        "      buttonStartAgain = types.KeyboardButton(\"Начать проходить заново\")\n",
        "      markup.row(buttonYes, buttonNo, buttonStartAgain)\n",
        "      bot.send_message(message.chat.id, f\"{questions[question_num]}\",reply_markup=markup)\n",
        "\n",
        "    # Открываем БД с данными о пользователе\n",
        "    elif message.content_type == 'text' and (message.text == \"Да\" or message.text == \"Нет\"):\n",
        "      specific_id, db = get_user_id_in_base(user_id)\n",
        "      question_num = list(db['question'])[specific_id]\n",
        "      hypothyroidism, insulinresistance, irondeficit = get_data(user_id, db, specific_id)\n",
        "\n",
        "      if question_num < 33:\n",
        "        if message.text == \"Да\":\n",
        "          if question_num == 2:\n",
        "            irondeficit += 1\n",
        "          if question_num < 9:\n",
        "            hypothyroidism += 1\n",
        "          elif 9 <= question_num < 21:\n",
        "            insulinresistance += 1\n",
        "          elif 22 <= question_num < 34:\n",
        "            irondeficit += 1\n",
        "          question_num += 1\n",
        "        else:\n",
        "          question_num += 1\n",
        "        # На последнем вопросе уходим сюда, и только тогда он его засчитает\n",
        "        if question_num == 33:\n",
        "          db = db.drop(db.index[specific_id])\n",
        "          db.to_csv(\"user_data.csv\", index=False)\n",
        "          with open('user_data.csv', 'a') as f:\n",
        "            f.write(f\"{user_id}, {1}, {question_num}, {hypothyroidism}, {insulinresistance}, {irondeficit}\\n\")\n",
        "            db = pd.read_csv('user_data.csv')\n",
        "          markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "          agreeButton = types.KeyboardButton('Узнать результаты')\n",
        "          markup.row(agreeButton)\n",
        "          bot.send_message(message.chat.id, f\"{questions[question_num]}\",reply_markup=markup)\n",
        "        else:\n",
        "          markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)\n",
        "          buttonYes = types.KeyboardButton('Да')\n",
        "          buttonNo = types.KeyboardButton('Нет')\n",
        "          buttonStartAgain = types.KeyboardButton(\"Начать проходить заново\")\n",
        "          markup.row(buttonYes, buttonNo, buttonStartAgain)\n",
        "          bot.send_message(message.chat.id, f\"{questions[question_num]}\",reply_markup=markup)\n",
        "\n",
        "        # Обновляем БД после каждого вопроса\n",
        "          db = db.drop(db.index[specific_id])\n",
        "          db.to_csv(\"user_data.csv\", index=False)\n",
        "          with open('user_data.csv', 'a') as f:\n",
        "            f.write(f\"{user_id}, {1}, {question_num}, {hypothyroidism}, {insulinresistance}, {irondeficit}\\n\")\n",
        "\n",
        "    # Блок, где мы выдаем результаты опросника\n",
        "    elif message.content_type == 'text' and message.text == \"Узнать результаты\":\n",
        "      specific_id, db = get_user_id_in_base(user_id)\n",
        "      hypothyroidism, insulinresistance, irondeficit = get_data(user_id, db, specific_id)\n",
        "      show_results(message, hypothyroidism, insulinresistance, irondeficit, specific_id, db, user_id)\n",
        "\n",
        "    elif message.content_type == 'text' and message.text == \"Понятно\":\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "      continueButton1 = types.KeyboardButton(\"Продолжить\")\n",
        "      markup.row(continueButton1)\n",
        "      bot.send_message(message.chat.id, \"Если Вы хотите узнать, какой комплекс БАДов Вам необходим,\\n\"\n",
        "                                        \"нужно сдать анализы. Продолжаем?\", reply_markup = markup)\n",
        "\n",
        "\n",
        "    elif message.content_type == 'text' and message.text == 'Продолжить':\n",
        "      markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "      continueButton2 = types.KeyboardButton(\"Прочитано\")\n",
        "      markup.row(continueButton2)\n",
        "      print_instructions(message)\n",
        "      bot.send_message(message.chat.id, \"Пожалуйста, обращайте внимание на единицы измерений, которые указаны в таблице.\"\n",
        "        \"В случае, когда Вы записываете десятичное число через точку и таблица меняет его на дату, запишите это число\"\n",
        "        \"через запятую. При записи чисел НЕ используйте пробелы(при указании диапазона используйте дефис).\", reply_markup=markup)\n",
        "    # Выдаем соответствующую табличку, куда записывать результаты\n",
        "    elif (message.content_type == 'text' or message.content_type == 'document') and message.text == \"Прочитано\":\n",
        "      specific_id, db = get_user_id_in_base(user_id)\n",
        "      table_num = list(db['table_num'])[specific_id]\n",
        "      bot.send_message(message.chat.id, \"В данной ниже таблице вместо нулей заполните Ваши результаты анализов.\"\n",
        "                       \"Удостоверьтесь, что Ферритин, ТТГ и инсулин заполнены!\")\n",
        "      if table_num == 1:\n",
        "        hypo = drive.CreateFile({'id': '1vGqPSv5AExcS3yvR0gC7KQ9_fvHMcS9e'})\n",
        "        hypo.GetContentFile('Анализы3.xlsx')\n",
        "        bot.send_document(message.chat.id, open('Анализы3.xlsx', 'rb'))\n",
        "      elif table_num == 2:\n",
        "        inresist = drive.CreateFile({'id': '1YN07Hl4jUtQj5r07_1ak7xiXkWX1AN22'})\n",
        "        inresist.GetContentFile('Анализы1.xlsx')\n",
        "        bot.send_document(message.chat.id, open('Анализы1.xlsx', 'rb'))\n",
        "      else:\n",
        "        irondef = drive.CreateFile({'id': '17UXKuhm_gkyF2tixi8SGea7S4Rhsz0mz'})\n",
        "        irondef.GetContentFile('Анализы2.xlsx')\n",
        "        bot.send_document(message.chat.id, open('Анализы2.xlsx', 'rb'))\n",
        "      bot.register_next_step_handler(message, get_analysis) #переделал из get_analysis в request_for_model потому что я тупой и ниче не понял и я заебался, потом исправлю\n",
        "\n",
        "    else:\n",
        "      bot.send_message(message.chat.id, \"Пожалуйста, отвечайте корректно. Используйте кнопки, предложенные под полем для ответа.\")\n",
        "# Получаем анализы от пользователя\n",
        "def get_analysis(message):\n",
        "  try:\n",
        "    file_name = message.document.file_name\n",
        "    result_table = bot.get_file(message.document.file_id)\n",
        "    downloaded_file = bot.download_file(result_table.file_path)\n",
        "    with open(file_name, 'wb') as new_file:\n",
        "      new_file.write(downloaded_file)\n",
        "    results_to_compare = pd.DataFrame(pd.read_excel(file_name))\n",
        "    markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "    manButton = types.KeyboardButton(\"Мужчина\")\n",
        "    womanButton = types.KeyboardButton(\"Женщина\")\n",
        "    markup.row(manButton, womanButton)\n",
        "    bot.send_message(message.chat.id, \"Для корректных результатов, необходимо узнать Ваш пол.\", reply_markup = markup)\n",
        "    bot.register_next_step_handler(message, process_analysis, results_to_compare)\n",
        "\n",
        "  except Exception:\n",
        "    bot.reply_to(message, \"Упс, произошла ошибка, попробуйте еще раз!\")\n",
        "    bot.register_next_step_handler(message, get_analysis)\n",
        "\n",
        "def process_analysis(message, results_to_compare):\n",
        "  result_base = pd.read_excel(\"Показания.xlsx\")\n",
        "  main_analysis = {}\n",
        "  for i in range(results_to_compare.count()[0]):\n",
        "    row = list(results_to_compare.loc[i])\n",
        "    if row[1] != 0 and not(pd.isnull(row[1])):\n",
        "      if row[0] == \"Инсулин\" or row[0] == \"ТТГ\" or row[0] == \"Ферритин\":\n",
        "        if row[1].is_integer() or isinstance(row[1], np.floating):\n",
        "          main_analysis[row[0]] = row[1]\n",
        "\n",
        "\n",
        "  # Саша, здесь есть повторяющиеся строки. Если у тебя есть идея, как от них избавиться, то давай :))) /// в ф-ию var_sender вынес все эти операции (она выше в комменнтариях).\n",
        "  if (\"Инсулин\" not in main_analysis) or (\"Ферритин\" not in main_analysis) or (\"ТТГ\" not in main_analysis):\n",
        "    bot.send_message(message.chat.id, \"Вы ввели некорректные данные в таблице, попробуйте еще раз!\"\n",
        "                     \"Удостоверьтесь, что Ферритин, ТТГ и инсулин заполнены!\")\n",
        "    bot.register_next_step_handler(message, get_analysis)\n",
        "  elif 50 <= main_analysis[\"Ферритин\"] <= 150 and 2 <= main_analysis[\"Инсулин\"] <= 6 and 0.4 <= float(main_analysis[\"ТТГ\"]) <= 2.0:\n",
        "    fourthvar = drive.CreateFile({'id': '1qmJapfl1Cx2G6RTAzEvCgCz047gEr8Dr'})\n",
        "    fourthvar.GetContentFile('Вариант4.pdf')\n",
        "    bot.send_document(message.chat.id, open(r\"Вариант4.pdf\", 'rb'))\n",
        "    bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "    bot.register_next_step_handler(message, ask_model)\n",
        "  else:\n",
        "    if main_analysis[\"Ферритин\"] <= main_analysis[\"Инсулин\"] or main_analysis[\"Инсулин\"] >= main_analysis[\"ТТГ\"]:\n",
        "      firstvar = drive.CreateFile({'id': '1t1yqo_6RSeE7Jp1DB11sd_2cCTsTj9_I'})    # замена на var_sender(message.chat.id, 1)\n",
        "      firstvar.GetContentFile('Вариант1.pdf')\n",
        "      bot.send_document(message.chat.id, open(r\"Вариант1.pdf\", 'rb'))\n",
        "      bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "      bot.register_next_step_handler(message, ask_model)\n",
        "    elif  main_analysis[\"Ферритин\"] >= main_analysis[\"ТТГ\"] or main_analysis[\"Ферритин\"] > main_analysis[\"Инсулин\"]:\n",
        "      secondvar = drive.CreateFile({'id': '15ULzx1xnjvafKmuR0wsLeQwHxAteVn9Z'})    # замена на var_sender(message.chat.id, 2)\n",
        "      secondvar.GetContentFile('Вариант2.pdf')\n",
        "      bot.send_document(message.chat.id, open(r\"Вариант2.pdf\", 'rb'))\n",
        "      bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "      bot.register_next_step_handler(message, ask_model)\n",
        "    else:\n",
        "      thirdvar = drive.CreateFile({'id': '1mkr7plbpyImrEtVrgaZtuwebi_xKFxrU'})    # замена на var_sender(message.chat.id, 3)\n",
        "      thirdvar.GetContentFile('Вариант3.pdf')\n",
        "      bot.send_document(message.chat.id, open(r\"Вариант3.pdf\", 'rb'))\n",
        "      bot.send_message(message.chat.id, \"Если у вас остались вопросы, можете задать их в свободной форме ниже, на них Вам ответит искусственный интеллект[beta].\\n\"\n",
        "    \"Начинайте спрашивать прямо сейчас. Если у Вас нет вопросов или Вы захотите остановиться, напишите СТОП.\")\n",
        "      bot.register_next_step_handler(message, ask_model)\n",
        "\n",
        "def ask_model(user_prompt: str):\n",
        "  message = user_prompt\n",
        "  if user_prompt.text.lower() != 'стоп':\n",
        "    user_prompt = translator.translate(user_prompt.text).text\n",
        "    output = pipe(f'''[INST] <<SYS>>\n",
        "    {system_prompt}<</SYS>>\n",
        "    {user_prompt}[/INST]''', return_full_text = False)[0]['generated_text']\n",
        "    translated_output = translator.translate(output, src=\"en\", dest=\"ru\")\n",
        "    bot.send_message(message.chat.id, translated_output.text)\n",
        "    bot.register_next_step_handler(message, ask_model)\n",
        "  else:\n",
        "    againButton = types.KeyboardButton(\"Начать проходить заново\")\n",
        "    markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard = True)\n",
        "    markup.row(againButton)\n",
        "    bot.send_message(message.chat.id, \"Вы можете начать проходить бота заново.\", reply_markup = markup)\n",
        "    bot.register_next_step_handler(message, sample)\n",
        "\n",
        "\n",
        "bot.infinity_polling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1RAvpjkMV_-",
        "outputId": "d67c36b7-f3c1-47ce-b5a7-d361cb6ea1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting telebot\n",
            "  Downloading telebot-0.0.5-py3-none-any.whl (4.8 kB)\n",
            "Collecting pyTelegramBotAPI (from telebot)\n",
            "  Downloading pyTelegramBotAPI-4.14.0.tar.gz (243 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.1/243.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from telebot) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2023.11.17)\n",
            "Building wheels for collected packages: pyTelegramBotAPI\n",
            "  Building wheel for pyTelegramBotAPI (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyTelegramBotAPI: filename=pyTelegramBotAPI-4.14.0-py3-none-any.whl size=215252 sha256=e12ccbe2187f000103b9c478414310d6dbb7a1feafda4f344c2377f5db5ecb7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/51/2d/24b40a366c85c37928d5aa36ddf257e5a79fad25e1ecd11b2c\n",
            "Successfully built pyTelegramBotAPI\n",
            "Installing collected packages: pyTelegramBotAPI, telebot\n",
            "Successfully installed pyTelegramBotAPI-4.14.0 telebot-0.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install telebot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WT_DDVGqHnC"
      },
      "source": [
        " **НЕЙРОНКА**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ovt-mUntgh",
        "outputId": "bfbdd2b1-67af-487d-cbe4-5ca4dbc500cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qqq install accelerate bitsandbytes transformers optimum googletrans==3.1.0a0\n",
        "!pip install -qqq auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Gy0uwJp_43ZI",
        "outputId": "35d0a59d-1dad-4fe8-e3cc-856220e51359"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b9bf170f5208>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m from transformers import (\n\u001b[1;32m      4\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    GPTQConfig\n",
        ")\n",
        "from google.colab import drive\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "drive.mount('/content/drive')\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "9dc5e9f2dcee4a21b208462946e4c2cd",
            "12c309b6981c46b699bf8029e7dd3887",
            "33675f8dafaa44768562bcef2c456485",
            "e8575e253a2d41718ce8879840e4fcc4",
            "73aeed0db15b48cfb2678415524978bd",
            "66dee0cdbd12452699845be77fdb55f5",
            "59e18ce1db274f859ddec213e92a1ed2",
            "96e695b198b64993a3a9568da05cd8d3",
            "26e9501a5d1d483ca43a24a302f0854d",
            "a09f218ef7594790bc59de3dd6e56833",
            "996ce67990694b9d8d4f9270bcd2386d",
            "9594d9c4ab11489e844291430cb21cc2",
            "7378b568c7fc435e8ace7fb29b9ea601",
            "a25452d9769144e6831bf3cec12668ea",
            "1c4980ad542543f89afbe24c0b44df54",
            "cc39c5a2bf784566938e7883a0b488a7",
            "9764bef87d38498cbfaa6c7e8027acea",
            "2a082736c36a43d8b34df6bb60618909",
            "77b78f3fe4674a95b1089b1ebc17f796",
            "b9fb2917cddb4866a6280e72d60f4c76",
            "13de789a2999468da38cf4e867e58289",
            "b431b61e8b00441eaf2c35332242f295"
          ]
        },
        "id": "H7e1zXvYoYAh",
        "outputId": "4edcb2eb-610b-4044-ec8a-16f39a6d37d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dc5e9f2dcee4a21b208462946e4c2cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9594d9c4ab11489e844291430cb21cc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-91fdd07e5a0d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0madapter_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/checkpoint-240'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(base_model_name,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                              \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                              quantization_config=GPTQConfig(bits=4,disable_exllama=True))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3478\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3480\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3481\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3482\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3868\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3869\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_fsdp_enabled_and_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3870\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   3871\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3872\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;31m# For backward compatibility with older versions of `accelerate`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SCB\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "### Инициализация модели ######################################################################################\n",
        "# Эта ячейка загружает модель в видеокарту, ни в коем случае ее нельзя запускать несколько раз за одну сессию,\n",
        "# иначе в видюху загрузится несколько моделей и в ней начнет закачниватся видеопамять,\n",
        "# если все таки нужно повторно запустить код, то сначала перезапустите среду чтобы освободить\n",
        "# память от прошлой модели.\n",
        "# По умолчанию модель занимает не более 10 ГБ VRAM, если занято больше значит вы что то делаете не так\n",
        "#####################################################################################################################\n",
        "\n",
        "base_model_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "adapter_model = '/content/drive/MyDrive/checkpoint-240'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
        "                                             device_map={\"\": 0},\n",
        "                                             quantization_config=GPTQConfig(bits=4,disable_exllama=True))\n",
        "model = PeftModel.from_pretrained(model, adapter_model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0XLZBV9kKfe",
        "outputId": "9a55b7b7-9156-4ef5-e5a3-7a0712799d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "### Параметры модели ###\n",
        "\n",
        "pipe = pipeline('text-generation',model=model,tokenizer=tokenizer,\n",
        "                max_new_tokens=300, # Максимальная длина ответа, сильное снижение может привести к обрезанию ответов\n",
        "                do_sample=True,   # не трогать\n",
        "                temperature=0.1,  # рандомность ответа, чем больше это число тем более неожиданные и креативные ответы будет давать модель, если число маленькое то ответы будут однообразными но уверенными.\n",
        "                top_p=0.95, #  тоже рандомность, чем меньше тем больше модель будет отходить от контекста.\n",
        "                top_k=40, # аналагично параметру выше, не рекомендуется менять.\n",
        "                repetition_penalty=1.15  # штраф за повторение, чем выше тем меньше модель будет повторять одни и те же слова/предложения, слишком высокое значение может привести к бреду/галлюцинациям\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjQ735UfqU_U"
      },
      "outputs": [],
      "source": [
        "### Системный промпт, в нем вы обьясняете боту кто он по жизни и в чем его цель, что ему стоит делать а что нет, можно свободно изменять.\n",
        "system_prompt = \"You are a helpful, respectful and honest assistant, you answer questions regarding food supplements and vitamins. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Answer the topic of the question, do not deviate from the topic of the question, do not philosophize, be precise and concise, use less than 150 words. Don't use any diagnoses or statistics in your answer. Use as few medical terms as possible. Do not ask questions back. Do not advise any vitamins.\"\n",
        "\n",
        "def ask_model(user_prompt: str):\n",
        "  user_prompt = translator.translate(user_prompt).text\n",
        "  output = pipe(f'''[INST] <<SYS>>\n",
        "  {system_prompt}<</SYS>>\n",
        "  {user_prompt}[/INST]''', return_full_text = False)[0]\n",
        "  translated_output = translator.translate(output, src=\"en\", dest=\"ru\")\n",
        "  return translated_output.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAT9bun0wgu1",
        "outputId": "86d12296-530c-4819-a91e-aa8cb4a1b49e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'generated_text': 'Я не могу назначать лекарства или рекомендовать дозировки. Однако если эта проблема возникает у вас каждый день, необходимо обращаться к специалисту (отоларингологу, аллергологу, проконсультироваться у этих врачей). Причину чихания может устранить врач. Во время такого наблюдения важно соблюдать правила гигиены: избегать контактов с людьми, больными ОРВИ, регулярно мыть руки, носить маску, защищаться от загрязнителей окружающей среды – здесь часто прячутся бактерии и вирусы. Если чихание возникает периодически, желательно принять Иммуцет плюс (продается в аптеке). Этот препарат основан на гомеопатии, поэтому не рекомендуется использовать его, если вы являетесь последователем индуизма или буддизма. Кроме того, прежде чем вынимать, проконсультируйтесь со специалистом. Если чихание сопровождается покраснением слизистой носа или гнойными выделениями, необходимо срочно обратиться за помощью к отоларингологу. При чихании необходимо соблюдать правила гигиены, избегать самолечения. Вы не можете решить свою проблему? Тогда обратитесь к специалисту! Вот несколько советов при чихании: • старайтесь держать голову в тепле, • используйте увлажнитель воздуха в квартире, • используйте смазку с соленой водой'}\n"
          ]
        }
      ],
      "source": [
        "print(ask_model('Как лечить хроническое чихание?'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12c309b6981c46b699bf8029e7dd3887": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66dee0cdbd12452699845be77fdb55f5",
            "placeholder": "​",
            "style": "IPY_MODEL_59e18ce1db274f859ddec213e92a1ed2",
            "value": "config.json: 100%"
          }
        },
        "13de789a2999468da38cf4e867e58289": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4980ad542543f89afbe24c0b44df54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13de789a2999468da38cf4e867e58289",
            "placeholder": "​",
            "style": "IPY_MODEL_b431b61e8b00441eaf2c35332242f295",
            "value": " 7.26G/7.26G [01:10&lt;00:00, 142MB/s]"
          }
        },
        "26e9501a5d1d483ca43a24a302f0854d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a082736c36a43d8b34df6bb60618909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33675f8dafaa44768562bcef2c456485": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e695b198b64993a3a9568da05cd8d3",
            "max": 837,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26e9501a5d1d483ca43a24a302f0854d",
            "value": 837
          }
        },
        "59e18ce1db274f859ddec213e92a1ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66dee0cdbd12452699845be77fdb55f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7378b568c7fc435e8ace7fb29b9ea601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9764bef87d38498cbfaa6c7e8027acea",
            "placeholder": "​",
            "style": "IPY_MODEL_2a082736c36a43d8b34df6bb60618909",
            "value": "model.safetensors: 100%"
          }
        },
        "73aeed0db15b48cfb2678415524978bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b78f3fe4674a95b1089b1ebc17f796": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9594d9c4ab11489e844291430cb21cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7378b568c7fc435e8ace7fb29b9ea601",
              "IPY_MODEL_a25452d9769144e6831bf3cec12668ea",
              "IPY_MODEL_1c4980ad542543f89afbe24c0b44df54"
            ],
            "layout": "IPY_MODEL_cc39c5a2bf784566938e7883a0b488a7"
          }
        },
        "96e695b198b64993a3a9568da05cd8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9764bef87d38498cbfaa6c7e8027acea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996ce67990694b9d8d4f9270bcd2386d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc5e9f2dcee4a21b208462946e4c2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12c309b6981c46b699bf8029e7dd3887",
              "IPY_MODEL_33675f8dafaa44768562bcef2c456485",
              "IPY_MODEL_e8575e253a2d41718ce8879840e4fcc4"
            ],
            "layout": "IPY_MODEL_73aeed0db15b48cfb2678415524978bd"
          }
        },
        "a09f218ef7594790bc59de3dd6e56833": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a25452d9769144e6831bf3cec12668ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b78f3fe4674a95b1089b1ebc17f796",
            "max": 7259449480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9fb2917cddb4866a6280e72d60f4c76",
            "value": 7259449480
          }
        },
        "b431b61e8b00441eaf2c35332242f295": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9fb2917cddb4866a6280e72d60f4c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc39c5a2bf784566938e7883a0b488a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8575e253a2d41718ce8879840e4fcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a09f218ef7594790bc59de3dd6e56833",
            "placeholder": "​",
            "style": "IPY_MODEL_996ce67990694b9d8d4f9270bcd2386d",
            "value": " 837/837 [00:00&lt;00:00, 29.0kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}